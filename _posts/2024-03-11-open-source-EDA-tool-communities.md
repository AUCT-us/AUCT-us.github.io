---
layout: post
title:  "Open Source Development Communities For EDA Tools"
date:   2024-03-11 4:30:00
categories: template
---

Let's say that you wanted to become some sort of [hardware verification engineer for company like NVIDIA](https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite?q=hardware%20verification&jobFamilyGroup=0c40f6bd1d8f10ae43ffaefd46dc7e78) ... you will have to go a little bit *beyond just being able to regurgitate Wikipedia pages for terms such as* [system-level](https://en.wikipedia.org/wiki/High-level_synthesis) [RTL](https://en.wikipedia.org/wiki/Register-transfer_level), [SystemVerilog](https://en.wikipedia.org/wiki/SystemVerilog), [Universal Verification Methodology](https://en.wikipedia.org/wiki/Universal_Verification_Methodology), [Synopsys VCS](https://www.synopsys.com/verification/simulation/vcs.html) or equivalent [HDL simulation tools](https://en.wikipedia.org/wiki/List_of_HDL_simulators) like [MentorGraphics ModelSim](https://en.wikipedia.org/wiki/ModelSim), [Debussy debug tool](https://www.design-reuse.com/news/1659/novas-open-source-web-tool.html), [the GNU Debugger](https://www.baeldung.com/linux/gdb-debug) for [debugging during hardware verification](https://interrupt.memfault.com/blog/gdb-for-firmware-1).

You kind of need to understand the how/why/what of electronic design automation (EDA) toolchains, including just enough of the history to be dangerous, ie to be able to understand the context of the tools that you will be using. As metal–oxide–semiconductor (MOS) technology progressed, millions and then billions of MOS transistors could be placed on one chip -- which means, that well before, [we entered the billion-transistor processor era over 20 years ago](https://web.archive.org/web/20110608072423/http://www.eetimes.com/electronics-products/processors/4079511/Intel-enters-billion-transistor-processor-era), integrated circuit designs really ***required*** the kind of exhaustively thorough planning and simulation tasks that would be impossible without modern EDA toolchains.

Now ... let's say ... just for the sake of arguement that you were interested in help people get up to speed with open source EDA tools ... knowing full well, that maximizing productivity from the best of the best designers will warrant the top-of-the-line proprietary EDA tools ... but also knowing that the open source EDA tools are ***good enough*** for DEVELOPING A BASIC UNDERSTANDING OF ALMOST ALL OF THE WORK that will needs to be done.

It's probably a good idea to BEGIN our discussion of the open source development communities that surround different open source EDA tools with [a ***very-high level*** **introduction** to the basic EDA design flow](https://anysilicon.com/asic-design-flow-ultimate-guide/) that is typically used for [Very Large Scale Integrated Circuits](https://en.wikipedia.org/wiki/Very_Large_Scale_Integration), [Very High Speed Integrated Circuits](https://en.wikipedia.org/wiki/Very_High_Speed_Integrated_Circuit_Program), [Ultra Large-Scale Integrated Circuits](https://en.wikipedia.org/wiki/Integrated_circuit#ULSI,_WSI,_SoC_and_3D-IC), [Wafer-Scale Integration](https://en.wikipedia.org/wiki/Wafer-scale_integration), [System on a Chip](https://en.wikipedia.org/wiki/System_on_a_chip), [3D Integrated Circuits](https://en.wikipedia.org/wiki/Three-dimensional_integrated_circuit), [Multi-chip Modules](https://en.wikipedia.org/wiki/Multi-chip_module), [Flip Chip Multi-Chip Modules](https://en.wikipedia.org/wiki/Flip_chip) and an increasing variety of hardware ... especially for anyone who has not had much experience in the realm of using an [EDA design flow](https://en.wikipedia.org/wiki/Design_flow_(EDA)) for producing any kind of [integrated circuit](https://en.wikipedia.org/wiki/Integrated_circuit) such as microprocessors (CPUs), graphical processors (GPUs), tensor processors (TPUs), AI accelerators, ASICs, FPGAs and the vast array of other large combinations of integrated circuits.

IT's really easy to get lost way deep in the weeds of the EDA design flow ... especially since the landscape evolves and evolves rather rather rapidly ... so let's just start with what will always be the commonalities ... what has to underpin all of the work that gets done ... even with AI coding assistants and other AI tools to handle the gigantic volume of tedious chores and checks that have to be done, ie trillions or more tasks per day or per hour.

Proficiency with [a debugger](https://spectralops.io/blog/top-11-debuggers-you-need-to-know-in-2021/), probably [the open source GNU Project Debugger, GDB](https://www.sourceware.org/gdb/), and the advanced tools of debugging, is always going to be essential ... the old expression "Code for 6 minutes. Debug for 6 hours" might be more like "AI Code for 6 seconds. Debug for 6 days" with AI-enabled tools ... even when we automate the creation things, testing and debugging are always going to be essential, although the nature of how we use AI in test and debug tools will obviously changes with AI assistants in an EDA design flow.

The importance of debugging means that the ability to understand the mathematical software libraries that underpin the EDA tools is always going to be essential. Underneath ALL of these executable specifications and EDA tools are going to be the mathematical software libraries that are the foundations of all features that are implemented in the more easily-used features that drive productivity in designers. [The Trilinos Project](https://trilinos.github.io/) is one such effort to facilitate the design, development, integration and ongoing support of mathematical software libraries. Other notable players in the field include [Portable, Extensible Toolkit for Scientific Computation (PETSc), *pronounced PET-see*](https://petsc.org/release/), [SUNDIALS and the Hypre Project: Exascale-capable libraries for adaptive time-stepping and scalable solvers](https://www.exascaleproject.org/highlight/sundials-and-hypre-exascale-capable-libraries-for-adaptive-time-stepping-and-scalable-solvers/) as well as [Extreme-Scale Scientific Software Stack(E4S)](https://www.exascaleproject.org/new-extreme-scale-scientific-software-stack-e4s-release-delivers-expanded-features-and-capabilities-e4s-version-24-02-now-available/) of the [US Dept Of Energy's Exascale Computing Project](https://www.exascaleproject.org/about/), [Multiphysics Object Oriented Simulation Environment (MOOSE)](https://mooseframework.inl.gov/), [Open Source Field Operation and Manipulation (OpenFoam)](https://openfoam.org/) for computational fluid dynamics, [Distributed and Unified Numerics Environment(DUNE)](https://www.dune-project.org/) an [other numerical libraries](https://en.wikipedia.org/wiki/List_of_numerical_libraries).


Since expertise in the application domain for different forms of sensors and instruments that interact with the world are going to important ... scripting languages and numerical data science *scratchpads* are going to be a necessary part of the toolkit ... and we should ***also*** mention the growing set of [different Python libraries that are used for scientific computing](https://www.stxnext.com/blog/most-popular-python-scientific-libraries/), including the *old* favorites, NumPy, SciPy, Matplotlib, Pandas, SymPy, Jupyter Notebooks as well as the PyTorch, TensorFlow, Keras, Scikit-learn ecoystems ... and the ***new*** proliferation of favorites *like Dask, Xarray, Holoviews, Bokeh, Plotly, Dash, Streamlit, Altair, Seaborn, Statsmodels, Scikit-image, NetworkX, Gensim, NLTK, Spacy, TextBlob, Pattern, [PyMC3](https://github.com/pymc-devs/pymc), Edward, Pyro, ArviZ, Theano, Lasagne, Pymc-learn, et al*  This set of different data science scripting tools will NEVER stop expanding ... ***IT CAN'T*** ... because the application domain for sensors and instruments that interact with the world is always going to be GROWING, changing and the set of new uses will always be EXPANDING.