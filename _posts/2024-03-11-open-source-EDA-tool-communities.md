---
layout: post
title:  "Open Source Development Communities For EDA Tools"
date:   2024-03-11 4:30:00
categories: template
---

Let's say that you wanted to become some sort of [hardware verification engineer for company like NVIDIA](https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite?q=hardware%20verification&jobFamilyGroup=0c40f6bd1d8f10ae43ffaefd46dc7e78) ... you will have to go a little bit *beyond just being able to regurgitate Wikipedia pages for terms such as* [system-level](https://en.wikipedia.org/wiki/High-level_synthesis) [RTL](https://en.wikipedia.org/wiki/Register-transfer_level), [SystemVerilog](https://en.wikipedia.org/wiki/SystemVerilog), [Universal Verification Methodology](https://en.wikipedia.org/wiki/Universal_Verification_Methodology), [Synopsys VCS](https://www.synopsys.com/verification/simulation/vcs.html) or equivalent [HDL simulation tools](https://en.wikipedia.org/wiki/List_of_HDL_simulators) like [MentorGraphics ModelSim](https://en.wikipedia.org/wiki/ModelSim), [Debussy debug tool](https://www.design-reuse.com/news/1659/novas-open-source-web-tool.html), [the GNU Debugger](https://www.baeldung.com/linux/gdb-debug) for [debugging during hardware verification](https://interrupt.memfault.com/blog/gdb-for-firmware-1).

You kind of need to understand the how/why/what of electronic design automation (EDA) toolchains, including just enough of the history to be dangerous, ie to be able to understand the context of the tools that you will be using. As metal–oxide–semiconductor (MOS) technology progressed, millions and then billions of MOS transistors could be placed on one chip -- which means, that well before, [we entered the billion-transistor processor era over 20 years ago](https://web.archive.org/web/20110608072423/http://www.eetimes.com/electronics-products/processors/4079511/Intel-enters-billion-transistor-processor-era), integrated circuit designs really ***required*** the kind of exhaustively thorough planning and simulation tasks that would be impossible without modern EDA toolchains.

Now ... let's say ... just for the sake of arguement that you were interested in help people get up to speed with open source EDA tools ... knowing full well, that maximizing productivity from the best of the best designers will warrant the top-of-the-line proprietary EDA tools ... but also knowing that the open source EDA tools are ***good enough*** for DEVELOPING A BASIC UNDERSTANDING OF ALMOST ALL OF THE WORK that will needs to be done.

It's probably a good idea to BEGIN our discussion of the open source development communities that surround different open source EDA tools with [a ***very-high level*** **introduction** to the basic EDA design flow](https://anysilicon.com/asic-design-flow-ultimate-guide/) that is typically used for [Very Large Scale Integrated Circuits](https://en.wikipedia.org/wiki/Very_Large_Scale_Integration), [Very High Speed Integrated Circuits](https://en.wikipedia.org/wiki/Very_High_Speed_Integrated_Circuit_Program), [Ultra Large-Scale Integrated Circuits](https://en.wikipedia.org/wiki/Integrated_circuit#ULSI,_WSI,_SoC_and_3D-IC), [Wafer-Scale Integration](https://en.wikipedia.org/wiki/Wafer-scale_integration), [System on a Chip](https://en.wikipedia.org/wiki/System_on_a_chip), [3D Integrated Circuits](https://en.wikipedia.org/wiki/Three-dimensional_integrated_circuit), [Multi-chip Modules](https://en.wikipedia.org/wiki/Multi-chip_module), [Flip Chip Multi-Chip Modules](https://en.wikipedia.org/wiki/Flip_chip) and an increasing variety of hardware ... especially for anyone who has not had much experience in the realm of using an [EDA design flow](https://en.wikipedia.org/wiki/Design_flow_(EDA)) for producing any kind of [integrated circuit](https://en.wikipedia.org/wiki/Integrated_circuit) such as microprocessors (CPUs), graphical processors (GPUs), tensor processors (TPUs), AI accelerators, ASICs, FPGAs and the vast array of other large combinations of integrated circuits.

After we have gotten lost deep in the weeds without really trying, we can see WHY it is really so easy for people to get really badly lost way deep in the weeds of comparing this tool vs that tool the EDA design flow ... especially since the landscape is driven to be competitive and nobody is going to rest on their laurels, so the landscape evolves and evolves in an impossibly rapid fashion ... so let's GO BACK TO THE FOUNDATION and think about what will always be the commonalities ... what HAS TO underpin all of the work that gets done ... even with AI coding assistants and other AI tools to handle the gigantic volume of tedious chores and checks that have to be done, ie trillions or more tasks per day or per hour.

Proficiency with [a debugger](https://spectralops.io/blog/top-11-debuggers-you-need-to-know-in-2021/) is always going to be ESSENTIAL ... even though all will probably look like some derivative of [the open source GNU Project Debugger, GDB](https://www.sourceware.org/gdb/) and that includeds different advanced tools and information dashboards used in debugging and test.  Debugging and test will be an order of magnitude greater with more automation and intelligent EDA tools ... the old expression from 2020 "Code for 6 minutes. Debug for 6 hours" will be more like "Let AI generate code for 6 seconds. Debug for 6 days" with four dashboards full of output from different AI-enabled tools. It's sort of easy to predict ... when we automate the creation of things, testing and debugging are always going to be EVEN BIGGER part of the human-essential work, although the nature of how we use AI in test and debug tools will obviously changes with AI assistants in an EDA design flow.

The importance of debugging means that the ability to understand what's *happening under the hood* ... that means the mathematical software libraries that underpin the EDA tools ... being conversant with what those libraries do and how they do it is always going to be essential. Underneath ALL of these executable specifications and EDA tools are going to be the mathematical software libraries that are the foundations of all features that are implemented in the more easily-used features that drive productivity in designers. [The Trilinos Project](https://trilinos.github.io/) is one such effort to facilitate the design, development, integration and ongoing support of mathematical software libraries. Other notable players in the field include [Portable, Extensible Toolkit for Scientific Computation (PETSc), *pronounced PET-see*](https://petsc.org/release/), [SUNDIALS and the Hypre Project: Exascale-capable libraries for adaptive time-stepping and scalable solvers](https://www.exascaleproject.org/highlight/sundials-and-hypre-exascale-capable-libraries-for-adaptive-time-stepping-and-scalable-solvers/) as well as [Extreme-Scale Scientific Software Stack(E4S)](https://www.exascaleproject.org/new-extreme-scale-scientific-software-stack-e4s-release-delivers-expanded-features-and-capabilities-e4s-version-24-02-now-available/) of the [US Dept Of Energy's Exascale Computing Project](https://www.exascaleproject.org/about/), [Multiphysics Object Oriented Simulation Environment (MOOSE)](https://mooseframework.inl.gov/), [Open Source Field Operation and Manipulation (OpenFoam)](https://openfoam.org/) for computational fluid dynamics, [Distributed and Unified Numerics Environment(DUNE)](https://www.dune-project.org/) an [other numerical libraries](https://en.wikipedia.org/wiki/List_of_numerical_libraries).


Since expertise in the application domain for different forms of sensors and instruments that interact with the world are going to important ... scripting languages and numerical data science *scratchpads* are going to be a necessary part of the toolkit ... and we should ***also*** mention the growing set of [different Python libraries that are used for scientific computing](https://www.stxnext.com/blog/most-popular-python-scientific-libraries/), including the *old* favorites, NumPy, SciPy, Matplotlib, Pandas, SymPy, Jupyter Notebooks as well as the PyTorch, TensorFlow, Keras, Scikit-learn ecoystems ... and the ***new*** proliferation of favorites *like Dask, Xarray, Holoviews, Bokeh, Plotly, Dash, Streamlit, Altair, Seaborn, Statsmodels, Scikit-image, NetworkX, Gensim, NLTK, Spacy, TextBlob, Pattern, [PyMC3](https://github.com/pymc-devs/pymc), Edward, Pyro, ArviZ, Theano, Lasagne, Pymc-learn, et al*  This set of different data science scripting tools will NEVER stop expanding ... ***IT CAN'T*** ... because the application domain for sensors and instruments that interact with the world is always going to be GROWING, changing and the set of new uses will always be EXPANDING.