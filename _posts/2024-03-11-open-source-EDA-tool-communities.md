---
layout: post
title:  "Open Source Development Communities For EDA Tools"
date:   2024-03-11 4:30:00
categories: template
---

Let's say that you wanted to become some sort of [hardware verification engineer for company like NVIDIA](https://nvidia.wd5.myworkdayjobs.com/NVIDIAExternalCareerSite?q=hardware%20verification&jobFamilyGroup=0c40f6bd1d8f10ae43ffaefd46dc7e78) ... you will have to go a little bit *beyond just being able to regurgitate Wikipedia pages for terms such as* [system-level](https://en.wikipedia.org/wiki/High-level_synthesis) [RTL](https://en.wikipedia.org/wiki/Register-transfer_level), [SystemVerilog](https://en.wikipedia.org/wiki/SystemVerilog), [Universal Verification Methodology](https://en.wikipedia.org/wiki/Universal_Verification_Methodology), [Synopsys VCS](https://www.synopsys.com/verification/simulation/vcs.html) or equivalent [HDL simulation tools](https://en.wikipedia.org/wiki/List_of_HDL_simulators) like [MentorGraphics ModelSim](https://en.wikipedia.org/wiki/ModelSim), [Debussy debug tool](https://www.design-reuse.com/news/1659/novas-open-source-web-tool.html), [the GNU Debugger](https://www.baeldung.com/linux/gdb-debug) for [debugging during hardware verification](https://interrupt.memfault.com/blog/gdb-for-firmware-1).

You kind of need to understand the how/why/what of electronic design automation (EDA) toolchains, including just enough of the history to be dangerous, ie to be able to understand the context of the tools that you will be using. As metal–oxide–semiconductor (MOS) technology progressed, millions and then billions of MOS transistors could be placed on one chip -- which means, that well before, [we entered the billion-transistor processor era over 20 years ago](https://web.archive.org/web/20110608072423/http://www.eetimes.com/electronics-products/processors/4079511/Intel-enters-billion-transistor-processor-era), integrated circuit designs really ***required*** the kind of exhaustively thorough planning and simulation tasks that would be impossible without modern EDA toolchains.

Now ... let's say ... just for the sake of arguement that you were interested in help people get up to speed with open source EDA tools ... knowing full well, that maximizing productivity from the best of the best designers will warrant the top-of-the-line proprietary EDA tools ... but also knowing that the open source EDA tools are ***good enough*** for DEVELOPING A BASIC UNDERSTANDING OF ALMOST ALL OF THE WORK that will needs to be done.

It's probably a good idea to BEGIN our discussion of the open source development communities that surround different open source EDA tools with [a ***very-high level*** **introduction** to the basic EDA design flow](https://anysilicon.com/asic-design-flow-ultimate-guide/) that is typically used for [Very Large Scale Integrated Circuits](https://en.wikipedia.org/wiki/Very_Large_Scale_Integration), [Very High Speed Integrated Circuits](https://en.wikipedia.org/wiki/Very_High_Speed_Integrated_Circuit_Program), [Ultra Large-Scale Integrated Circuits](https://en.wikipedia.org/wiki/Integrated_circuit#ULSI,_WSI,_SoC_and_3D-IC), [Wafer-Scale Integration](https://en.wikipedia.org/wiki/Wafer-scale_integration), [System on a Chip](https://en.wikipedia.org/wiki/System_on_a_chip), [3D Integrated Circuits](https://en.wikipedia.org/wiki/Three-dimensional_integrated_circuit), [Multi-chip Modules](https://en.wikipedia.org/wiki/Multi-chip_module), [Flip Chip Multi-Chip Modules](https://en.wikipedia.org/wiki/Flip_chip) and an increasing variety of hardware ... especially for anyone who has not had much experience in the realm of using an [EDA design flow](https://en.wikipedia.org/wiki/Design_flow_(EDA)) for producing any kind of [integrated circuit](https://en.wikipedia.org/wiki/Integrated_circuit) such as microprocessors (CPUs), graphical processors (GPUs), tensor processors (TPUs), AI accelerators, ASICs, FPGAs and the vast array of other large combinations of integrated circuits.

After we have gotten lost deep in the weeds without really trying, we can see WHY it is really so easy for people to get really badly lost way deep in the weeds of comparing this tool vs that tool the EDA design flow ... especially since the landscape is driven to be competitive and nobody is going to rest on their laurels, so the landscape evolves and evolves in an impossibly rapid fashion ... so let's GO BACK TO THE FOUNDATION and think about what will always be the commonalities ... what HAS TO underpin all of the work that gets done ... even with AI coding assistants and other AI tools to handle the gigantic volume of tedious chores and checks that have to be done, ie trillions or more tasks per day or per hour.

Proficiency with [a debugger](https://spectralops.io/blog/top-11-debuggers-you-need-to-know-in-2021/) is always going to be ESSENTIAL ... even though all will probably look like some derivative of [the open source GNU Project Debugger, GDB](https://www.sourceware.org/gdb/) and that includeds different advanced tools and information dashboards used in debugging and test.  Debugging and test will be an order of magnitude greater with more automation and intelligent EDA tools ... the old expression from 2020 "Code for 6 minutes. Debug for 6 hours" will be more like "Let AI generate code for 6 seconds. Debug for 6 days" with four dashboards full of output from different AI-enabled tools. It's sort of easy to predict ... when we automate the creation of things, testing and debugging are always going to be EVEN BIGGER part of the human-essential work, although the nature of how we use AI in test and debug tools will obviously changes with AI assistants in an EDA design flow.

The importance of debugging means that the ability to understand what's *happening under the hood* ... because underneath ALL of these executable specifications and EDA tools are going to be the mathematical software libraries that are the foundations of all features that are implemented in the more easily-used features that drive productivity of the integrated circuit designers. Any modern scientific or machine learning application or some compute-intensive part of the EDA chain will require an entangled combination of [message passing channels](https://en.wikipedia.org/wiki/Message_passing) (e.g., [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface)), [multi-threading](https://www.geeksforgeeks.org/difference-between-multitasking-multithreading-and-multiprocessing/) which might be [CPU-specific hyper-threading](https://web.archive.org/web/20100821074918/http://cache-www.intel.com/cd/00/00/01/77/17705_htt_user_guide.pdf), and GPU-specific programming
(e.g., CUDA) to execute.

That means having at least a passing familiarity with mathematical software libraries that underpin the EDA tools ... being conversant with what those libraries do, how they do it is always going to be essential -- just as having [professional skills in programming](https://www.tiobe.com/tiobe-index/) is always going to be in demand somewhere, not quite, but almost in the same manner that [having professional level skills in a trade](https://www.indeed.com/career-advice/finding-a-job/highest-paying-trade-jobs) like [being electrician](https://www.indeed.com/career-advice/career-development/how-to-become-an-electrician) or [being a plumber](https://www.indeed.com/career-advice/career-development/how-to-become-a-plumber) is going to be important ... the skills that really matter are in **integration** and that usually means being able to understand the details, but see the larger picture and **quickly SOLVE the whole problem in a cost-effective, standards-compliant manner** ... metaphorically speaking, it's just about the porcelain interface or the visible fixtures, but also in getting the right pipes to the fixtures and also ensuring that the drains actually drain and stay unclogged. It's really about the big picture and solving the root cause of the problem, rather than *shooting from the hip* and *just part-swapping because you can swap a part that'll fit* ... this will involve being able to understand the particular stakeholder needs of the larger situation as well as the budget, resources and time avaiable well enough to stabilize the situation as well as develop a longer-term project work breakdown structure, even if that means involving qualified subcontractors and/or a general contractor. Familiarity with mathematical software libraries and standards is *just like* plumbing ... **don't underestimate how impossibly tough plumbing can be [especially when it has been done wrong], but realize that there's a way to do it AND a way that it has to be done to interoperate with the rest of the world.**


Understanding mathematical software libraries includes knowing something about the state of recent activity happening in the libraries as well as something about where the standards are moving in order to [use a language like C++ or C WELL](https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#S-cpl). [The Trilinos Project](https://trilinos.github.io/) is one such effort to facilitate the design, development, integration and ongoing support of mathematical software libraries and it illustrates how these different projects work together [or where the fragilities creep in from shared dependencies upon certain projects]; these projects tend to fork, but they do not really re-invent the wheels *mostly*. Trilinos, for example relies heavily on things like the Exascale Project's [Kokkos ecosystem](https://kokkos.org/) and Kokkos C++ Performance Portability Programming [Kokkos Kernels](https://github.com/kokkos/kokkos-kernels) implements **local**, ie not using MPI, computational kernels for linear algebra and graph operations, using the Kokkos shared-memory parallel programming model. Conversely, the [Kokkos Core](https://github.com/kokkos/kokkos) implements a programming model in C++ for writing performance portable applications targeting all major HPC platforms [using MPI]. For that purpose it provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. Kokkos currently can use [NVIDIA's CUDA](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html), [AMD's ROCm HIP](https://github.com/ROCm/HIP), [SYCL](https://www.khronos.org/sycl/), [HPX](https://hpx-docs.stellar-group.org/branches/master/html/index.html#what-is-hpx), [OpenMP](https://www.openmp.org/) ... and others that use [C++ threads](https://www.geeksforgeeks.org/multithreading-in-cpp/) as backend programming models with several other backends under development. 

Other notable players in the field of mathematical software libraries used in high-performance compute-intensive scientific programming includes [SUNDIALS and the Hypre Project: Exascale-capable libraries for adaptive time-stepping and scalable solvers](https://www.exascaleproject.org/highlight/sundials-and-hypre-exascale-capable-libraries-for-adaptive-time-stepping-and-scalable-solvers/) as well as [Extreme-Scale Scientific Software Stack(E4S)](https://www.exascaleproject.org/new-extreme-scale-scientific-software-stack-e4s-release-delivers-expanded-features-and-capabilities-e4s-version-24-02-now-available/) of the [US Dept Of Energy's Exascale Computing Project](https://www.exascaleproject.org/about/), [Portable, Extensible Toolkit for Scientific Computation (PETSc), *pronounced PET-see*](https://petsc.org/release/), [Multiphysics Object Oriented Simulation Environment (MOOSE)](https://mooseframework.inl.gov/), [Open Source Field Operation and Manipulation (OpenFoam)](https://openfoam.org/) for computational fluid dynamics, [Distributed and Unified Numerics Environment(DUNE)](https://www.dune-project.org/) ... and hosts of [other numerical libraries](https://en.wikipedia.org/wiki/List_of_numerical_libraries).


Since **expertise in the application domain** for different forms of sensors and instruments that interact with the world is going to important ... the need for scripting languages and numerical data science *scratchpads* will be even more necessary than now, just as part of the toolkit that's taken for granted, ie maybe more essential than being able to use a whiteboard or pen/notepad ... accordingly, we should ***also*** mention the growing set of [different Python libraries that are used for scientific computing](https://www.stxnext.com/blog/most-popular-python-scientific-libraries/), including the *old* favorites, NumPy, SciPy, Matplotlib, Pandas, SymPy, Jupyter Notebooks as well as the PyTorch, TensorFlow, Keras, Scikit-learn ecoystems ... and the ***new*** proliferation of favorites *like Dask, Xarray, Holoviews, Bokeh, Plotly, Dash, Streamlit, Altair, Seaborn, Statsmodels, Scikit-image, NetworkX, Gensim, NLTK, Spacy, TextBlob, Pattern, [PyMC3](https://github.com/pymc-devs/pymc), Edward, Pyro, ArviZ, Theano, Lasagne, Pymc-learn, et al*  This set of different data science scripting tools will NEVER stop expanding ... ***IT CAN'T*** ... because the application domain for sensors and instruments that interact with the world is always going to be GROWING, changing and the set of new uses will always be EXPANDING.